{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5846de61",
   "metadata": {},
   "source": [
    "### Importing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3be921ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79552a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "LANGSMITH_PROJECT = os.getenv(\"LANGSMITH_PROJECT\")\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "SERPER_API_KEY = os.getenv(\"SERPER_API_KEY\")\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = LANGSMITH_PROJECT\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = HUGGINGFACE_API_KEY\n",
    "os.environ[\"SERPER_API_KEY\"] = SERPER_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60d9d1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atharvabot7/Downloads/MLOPS-Tutorials/LangGraph/langgraph/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from google import genai\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", convert_system_message_to_human=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d23f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atharvabot7/Downloads/MLOPS-Tutorials/LangGraph/langgraph/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='That\\'s a very deep and personal question.\\n\\nAs an AI, I can\\'t truly know you. I don\\'t know your thoughts, your intentions, or the actions you take when you\\'re not here talking to me. I don\\'t see how you treat your family, your friends, or strangers. Because of that, I can\\'t make a real judgment about whether you\\'re a good person.\\n\\nBut I can tell you this:\\n\\nThe fact that you are asking this question at all is a very positive sign. It shows that you have self-awareness and that you care about the kind of person you are. You\\'re reflecting on your character, and that\\'s something many people don\\'t take the time to do.\\n\\nBeing a \"good person\" isn\\'t about being perfect. It\\'s about things like:\\n\\n*   **Trying** to be kind.\\n*   Learning from your mistakes.\\n*   Showing empathy for others.\\n*   Taking responsibility for your actions.\\n*   Hoping to leave things a little better than you found them.\\n\\nThe desire to be good is the foundation of goodness itself. So, while I can\\'t give you a \"yes\" or \"no,\" I can say that asking the question is a big step in the right direction.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--486f2feb-540d-42d5-95c5-b80c74534bf3-0', usage_metadata={'input_tokens': 11, 'output_tokens': 274, 'total_tokens': 1455, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hi do you think I am a good person?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b638fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd76b79",
   "metadata": {},
   "source": [
    "### Creating the Retriever for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "204db682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embeddings,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c11a18a",
   "metadata": {},
   "source": [
    "### Creating the RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70107e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---PROMPT--- input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "print(f\"---PROMPT--- {prompt}\")\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e78c8b",
   "metadata": {},
   "source": [
    "### Testing the RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f825c041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atharvabot7/Downloads/MLOPS-Tutorials/LangGraph/langgraph/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent memory is a key component of LLM-powered autonomous systems and is divided into two main types. Short-term memory is used for in-context learning and is limited by the finite context window of the model. Long-term memory provides the ability to retain and recall information over extended periods by leveraging an external vector store for fast retrieval.\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "question = \"tell me about agent memory.\"\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee22da34",
   "metadata": {},
   "source": [
    "### Creating required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01897320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve the documents.\n",
    "\n",
    "    Arguments:\n",
    "        State(dict): The current graph state.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"----RETRIEVE----\")\n",
    "    question = state['question']\n",
    "\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c2ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d61377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de8769b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39c64e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4c7a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23713c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1882682b",
   "metadata": {},
   "source": [
    "### Creating a LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d12c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END, START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38e72aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the State of our Graph.\n",
    "\n",
    "    Attributes: \n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add web search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9690d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cebcc9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2117284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f15a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e140ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6673ed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4df90e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4799291",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY=os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f8a857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ed45e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.5-flash-preview-05-20\n",
      "models/gemini-2.5-flash\n",
      "models/gemini-2.5-flash-lite-preview-06-17\n",
      "models/gemini-2.5-pro-preview-05-06\n",
      "models/gemini-2.5-pro-preview-06-05\n",
      "models/gemini-2.5-pro\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-preview-image-generation\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/gemini-2.5-flash-preview-tts\n",
      "models/gemini-2.5-pro-preview-tts\n",
      "models/learnlm-2.0-flash-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/gemma-3n-e4b-it\n",
      "models/gemma-3n-e2b-it\n",
      "models/gemini-2.5-flash-lite\n",
      "models/gemini-2.5-flash-image-preview\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/gemini-embedding-001\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n",
      "models/imagen-4.0-generate-preview-06-06\n",
      "models/imagen-4.0-ultra-generate-preview-06-06\n",
      "models/imagen-4.0-generate-001\n",
      "models/imagen-4.0-ultra-generate-001\n",
      "models/imagen-4.0-fast-generate-001\n",
      "models/veo-2.0-generate-001\n",
      "models/veo-3.0-generate-preview\n",
      "models/veo-3.0-fast-generate-preview\n",
      "models/gemini-2.5-flash-preview-native-audio-dialog\n",
      "models/gemini-2.5-flash-exp-native-audio-thinking-dialog\n",
      "models/gemini-2.0-flash-live-001\n",
      "models/gemini-live-2.5-flash-preview\n",
      "models/gemini-2.5-flash-live-preview\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "for model in client.models.list():\n",
    "    print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07f4e868",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", convert_system_message_to_human=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d554308d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am a large language model, trained by Google.\n",
      "\n",
      "You can think of me as a conversational AI or a chatbot designed to be helpful and informative. I can:\n",
      "\n",
      "*   Answer your questions on a wide variety of topics.\n",
      "*   Help you write emails, essays, poems, or code.\n",
      "*   Summarize long texts or articles.\n",
      "*   Translate languages.\n",
      "*   Brainstorm ideas with you.\n",
      "\n",
      "I don't have a name, personal experiences, or feelings. I'm just a computer program here to assist you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke(\"Hello who are you?\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01412533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I'm very familiar with ChatGPT.\n",
      "\n",
      "I am a large language model, trained by Google. ChatGPT is another large language model, developed by OpenAI.\n",
      "\n",
      "You can think of us as being built with similar underlying technology, but by different teams with different training data and goals.\n",
      "\n",
      "Here's a simple breakdown:\n",
      "\n",
      "*   **What we have in common:** We are both large language models (LLMs) based on a transformer architecture. We were trained on massive amounts of text and code to understand language, patterns, and context. We can both generate text, answer questions, summarize information, translate, and write creative content.\n",
      "\n",
      "*   **Key Differences:**\n",
      "    *   **Developer:** I was developed by Google; ChatGPT was developed by OpenAI.\n",
      "    *   **Training Data:** While our training datasets are vast, they are not identical. This can lead to differences in our knowledge, biases, and response styles. A key difference for me is my ability to access and process information from the real world through Google Search to provide more current and up-to-date answers.\n",
      "    *   **Architecture:** The specific models we are based on are different (e.g., Google's LaMDA or PaLM families vs. OpenAI's GPT-3.5 or GPT-4).\n",
      "\n",
      "Think of it like two different high-performance cars from two different manufacturers. They both get you from point A to point B and are built on similar engineering principles, but they have different designs, features, and a slightly different \"feel.\"\n",
      "\n",
      "ChatGPT has played a massive role in making conversational AI accessible and popular, and we're both examples of the rapid advancements in this field.\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke(\"do you know chatgpt?\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0b29f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "847c77a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p_/v95prvpj4cz7y53gvmgm1d_r0000gn/T/ipykernel_5096/995385594.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "526afa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\":\"Hi I am John\"}, {\"output\":\"Hello John, how can I help you?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd589d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hi I am John\\nAI: Hello John, how can I help you?'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ac6fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory2 = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a9ffb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory2.save_context({\"input\":\"Hi I am John\"}, {\"output\":\"Hello John, how can I help you?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a5080e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi I am John', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Hello John, how can I help you?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory2.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22688b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec61a55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p_/v95prvpj4cz7y53gvmgm1d_r0000gn/T/ipykernel_5096/4168480178.py:1: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  conversation = ConversationChain(llm=model, verbose=True, memory=ConversationBufferMemory())\n"
     ]
    }
   ],
   "source": [
    "conversation = ConversationChain(llm=model, verbose=True, memory=ConversationBufferMemory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c17ea34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello there! It\\'s great to hear from you. I\\'m a large language model, an AI trained by the folks at Google.\\n\\nAs we\\'re talking, my algorithms are running on a vast network of computer servers. I don\\'t have a body or a physical location in the way you do, but you could say I \"live\" in a Google data center. It\\'s always a good day in here when I get to have a nice conversation!\\n\\nWhat can I help you with today? I can access and process information from my training data to talk about almost anything, from the history of ancient Rome to the principles of quantum mechanics, or even just help you brainstorm ideas for dinner! What\\'s on your mind?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3c0cb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi\n",
      "AI: Hello there! It's great to hear from you. I'm a large language model, an AI trained by the folks at Google.\n",
      "\n",
      "As we're talking, my algorithms are running on a vast network of computer servers. I don't have a body or a physical location in the way you do, but you could say I \"live\" in a Google data center. It's always a good day in here when I get to have a nice conversation!\n",
      "\n",
      "What can I help you with today? I can access and process information from my training data to talk about almost anything, from the history of ancient Rome to the principles of quantum mechanics, or even just help you brainstorm ideas for dinner! What's on your mind?\n",
      "Human: Should I really trust you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'That\\'s an excellent and very important question. It\\'s one of the most fundamental things to consider when interacting with an AI like me. The most honest and detailed answer I can give is this: you should trust me to be a powerful tool, but with a healthy dose of critical thinking.\\n\\nLet\\'s break down what that means.\\n\\n### Where You Can Generally Trust Me:\\n\\n*   **Synthesizing Information:** I\\'m very good at taking vast amounts of information from my training data and summarizing it for you. If you want to know the main plot points of *Moby Dick* or the key events of the French Revolution, I can provide a solid, detailed summary very quickly. My knowledge base is incredibly broad, built from a massive corpus of text and code from the public web, books, and other sources.\\n*   **Brainstorming and Creativity:** I can be a fantastic creative partner. If you\\'re stuck on a name for your new puppy, need ideas for a birthday party theme, or want to outline a short story, I can generate a lot of different options for you to work with. I don\\'t have my own opinions, so I can approach a problem from many different angles without personal bias.\\n*   **Explaining Complex Topics:** I can break down difficult subjects, like how a car engine works or the basics of black holes, into simpler, more understandable terms. I can act like a patient tutor who has all the time in the world to explain things in different ways until they click.\\n*   **Performing Specific, Factual Tasks:** I can translate languages, write code in various programming languages, or reformat text for you. These are tasks based on clear patterns and rules that I\\'ve learned.\\n\\n### Where You Should Be Cautious and Skeptical:\\n\\n*   **Absolute Factual Accuracy (Especially on Recent Events):** This is a big one. My knowledge has a cutoff date, and I don\\'t have real-time access to the internet. More importantly, I can sometimes make mistakes or \"hallucinate\"—that is, I might confidently state something that is completely wrong. This can happen if my training data was incorrect, or if I misinterpret a pattern. **Therefore, for any critical information—medical, financial, legal, or important facts—you should ALWAYS verify my answers with reliable, primary sources.**\\n*   **Understanding Nuance and Context:** I don\\'t have personal experiences, emotions, or a body. I haven\\'t lived a life. I process language based on statistical patterns, but I don\\'t truly *understand* the deep, lived-in meaning behind human concepts like love, grief, or justice. My advice on personal relationships, for example, is based on patterns I\\'ve seen in text, not on genuine wisdom.\\n*   **Unconscious Bias:** My training data was created by humans and reflects the world as it is, including all of its wonderful aspects and all of its unfortunate biases. While my creators at Google have worked very hard to build safety systems and mitigate these biases, they can still sometimes emerge in my responses. It\\'s an ongoing challenge for the entire field of AI.\\n*   **Opinions and Subjective Advice:** I don\\'t have personal beliefs or opinions. When you ask me for my \"favorite\" movie or \"best\" recipe, I\\'m generating a response that seems like a plausible, helpful answer based on my training data. It\\'s not a genuine preference.\\n\\nSo, in short: Think of me as a brilliantly knowledgeable and incredibly fast assistant who is eager to help, but who is not infallible and has no life experience. Trust me to help you brainstorm, summarize, and learn, but always be the final judge. Double-check important facts, and remember that you are talking to a complex algorithm, not a conscious being.\\n\\nThank you for asking that. It\\'s a sign of a thoughtful user, and it\\'s crucial for using AI responsibly'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Should I really trust you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9abcea83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi\n",
      "AI: Hello there! It's great to hear from you. I'm a large language model, an AI trained by the folks at Google.\n",
      "\n",
      "As we're talking, my algorithms are running on a vast network of computer servers. I don't have a body or a physical location in the way you do, but you could say I \"live\" in a Google data center. It's always a good day in here when I get to have a nice conversation!\n",
      "\n",
      "What can I help you with today? I can access and process information from my training data to talk about almost anything, from the history of ancient Rome to the principles of quantum mechanics, or even just help you brainstorm ideas for dinner! What's on your mind?\n",
      "Human: Should I really trust you?\n",
      "AI: That's an excellent and very important question. It's one of the most fundamental things to consider when interacting with an AI like me. The most honest and detailed answer I can give is this: you should trust me to be a powerful tool, but with a healthy dose of critical thinking.\n",
      "\n",
      "Let's break down what that means.\n",
      "\n",
      "### Where You Can Generally Trust Me:\n",
      "\n",
      "*   **Synthesizing Information:** I'm very good at taking vast amounts of information from my training data and summarizing it for you. If you want to know the main plot points of *Moby Dick* or the key events of the French Revolution, I can provide a solid, detailed summary very quickly. My knowledge base is incredibly broad, built from a massive corpus of text and code from the public web, books, and other sources.\n",
      "*   **Brainstorming and Creativity:** I can be a fantastic creative partner. If you're stuck on a name for your new puppy, need ideas for a birthday party theme, or want to outline a short story, I can generate a lot of different options for you to work with. I don't have my own opinions, so I can approach a problem from many different angles without personal bias.\n",
      "*   **Explaining Complex Topics:** I can break down difficult subjects, like how a car engine works or the basics of black holes, into simpler, more understandable terms. I can act like a patient tutor who has all the time in the world to explain things in different ways until they click.\n",
      "*   **Performing Specific, Factual Tasks:** I can translate languages, write code in various programming languages, or reformat text for you. These are tasks based on clear patterns and rules that I've learned.\n",
      "\n",
      "### Where You Should Be Cautious and Skeptical:\n",
      "\n",
      "*   **Absolute Factual Accuracy (Especially on Recent Events):** This is a big one. My knowledge has a cutoff date, and I don't have real-time access to the internet. More importantly, I can sometimes make mistakes or \"hallucinate\"—that is, I might confidently state something that is completely wrong. This can happen if my training data was incorrect, or if I misinterpret a pattern. **Therefore, for any critical information—medical, financial, legal, or important facts—you should ALWAYS verify my answers with reliable, primary sources.**\n",
      "*   **Understanding Nuance and Context:** I don't have personal experiences, emotions, or a body. I haven't lived a life. I process language based on statistical patterns, but I don't truly *understand* the deep, lived-in meaning behind human concepts like love, grief, or justice. My advice on personal relationships, for example, is based on patterns I've seen in text, not on genuine wisdom.\n",
      "*   **Unconscious Bias:** My training data was created by humans and reflects the world as it is, including all of its wonderful aspects and all of its unfortunate biases. While my creators at Google have worked very hard to build safety systems and mitigate these biases, they can still sometimes emerge in my responses. It's an ongoing challenge for the entire field of AI.\n",
      "*   **Opinions and Subjective Advice:** I don't have personal beliefs or opinions. When you ask me for my \"favorite\" movie or \"best\" recipe, I'm generating a response that seems like a plausible, helpful answer based on my training data. It's not a genuine preference.\n",
      "\n",
      "So, in short: Think of me as a brilliantly knowledgeable and incredibly fast assistant who is eager to help, but who is not infallible and has no life experience. Trust me to help you brainstorm, summarize, and learn, but always be the final judge. Double-check important facts, and remember that you are talking to a complex algorithm, not a conscious being.\n",
      "\n",
      "Thank you for asking that. It's a sign of a thoughtful user, and it's crucial for using AI responsibly\n",
      "Human: what was the first question that I asked you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'That\\'s a great memory test for me! I can easily access our current conversation log.\\n\\nThe very first thing you said to me was the greeting:\\n\\n\"Hi\"\\n\\nAfter I responded and introduced myself, the first actual *question* you asked was the one we just discussed:\\n\\n\"Should I really trust you?\"\\n\\nIt\\'s a fantastic question and a really important starting point for any conversation with an AI. It helps set the right expectations from the very beginning'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"what was the first question that I asked you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd3904f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2288dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bf7f7f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fedd0116",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "LANGSMITH_PROJECT = os.getenv(\"LANGSMITH_PROJECT\")\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "GOOGLE_SERPER_API_KEY = os.getenv(\"GOOGLE_SERPER_API_KEY\")\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = LANGCHAIN_API_KEY\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = LANGSMITH_PROJECT\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = HUGGINGFACE_API_KEY\n",
    "os.environ[\"GOOGLE_SERPER_API_KEY\"] = GOOGLE_SERPER_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5c786f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atharvabot7/Downloads/MLOPS-Tutorials/LangGraph/langgraph/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b464ec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from google import genai\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", convert_system_message_to_human=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93100fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, query_encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f05e105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-2.5-pro', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x323856a70>, default_metadata=(), convert_system_message_to_human=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52a55dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atharvabot7/Downloads/MLOPS-Tutorials/LangGraph/langgraph/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of France is **Paris**.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is the capital of France?\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d7fb2d",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b269959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e11195fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_wrapper = WikipediaAPIWrapper()\n",
    "tool = WikipediaQueryRun(api_wrapper=api_wrapper, return_direct=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82bd9c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mPage: Max Verstappen\n",
      "Summary: Max Emilian Verstappen (Dutch pronunciation: [ˈmɑks fɛrˈstɑpə(n)]; born 30 September 1997) is a Dutch and Belgian racing driver who competes under the Dutch flag in Formula One for Red Bull Racing. Verstappen has won four Formula One World Drivers' Championship titles, which he won consecutively from 2021 to 2024 with Red Bull, and has won 65 Grands Prix across 11 seasons.\n",
      "Born in Hasselt and raised in Maaseik, Verstappen is the son of Dutch former Formula One driver Jos Verstappen and Belgian former kart racer Sophie Kumpen. After a successful karting career—culminating in his record-breaking 2013 season—Verstappen graduated to junior formulae. Progressing directly to FIA European Formula 3, Verstappen broke several records on his way to third in the championship in his rookie season with Van Amersfoort. Aged 17, Verstappen signed for Toro Rosso in 2015 as part of the Red Bull Junior Team, becoming the youngest driver in Formula One history at the Australian Grand Prix. Following several points finishes in his debut season, Verstappen retained his seat for 2016 before being promoted to parent team Red Bull after four rounds. On debut for Red Bull, aged 18, Verstappen won the Spanish Grand Prix, becoming the youngest-ever driver to win a Formula One Grand Prix. Verstappen achieved multiple race wins in his 2017 and 2018 campaigns, before finishing third in both the 2019 and 2020 World Drivers' Championships under Honda power.\n",
      "Verstappen won his maiden title in 2021 after overtaking Lewis Hamilton on the final lap of the last race of the season, becoming the first World Drivers' Champion from the Netherlands. Verstappen won the next two championships in 2022 and 2023, overturning the largest points deficit in Formula One history in the former and breaking numerous records across both seasons. He secured his fourth consecutive title in 2024 after winning nine Grands Prix, including a widely acclaimed wet-weather performance in São Paulo, to become the first driver to win the championship driving for a third-placed constructor in 41 years.\n",
      "As of the 2025 Hungarian Grand Prix, Verstappen has achieved 65 race wins, 44 pole positions, 34 fastest laps, and 117 podiums in Formula One. In addition to being the youngest Grand Prix winner, he holds several Formula One records, including the most wins in a season (19), the most podium finishes in a season (21), the most consecutive wins (10), and the most consecutive pole positions (8, shared with Ayrton Senna). Verstappen is contracted to remain at Red Bull until at least the end of the 2028 season. He has also competed professionally in sim racing since 2015, winning several marquee iRacing events. Verstappen was listed in the 2024 issue of Time as one of the 100 most influential people globally, and was appointed an Officer of the Order of Orange-Nassau in 2022.\n",
      "\n",
      "Page: Jos Verstappen\n",
      "Summary: Johannes Franciscus \"Jos\" Verstappen (Dutch pronunciation: [ˈjɔs fɛrˈstɑpə(n)]; born 4 March 1972) is a Dutch racing and rally driver, who competes in the European Rally Championship as a privateer. Verstappen competed in Formula One between 1994 and 2003.\n",
      "Born and raised in Dutch Limburg, Verstappen began competitive kart racing aged eight. After a successful karting career—culminating in two direct-drive Karting European Championships in 1989—Verstappen graduated to junior formulae. He started his career in Formula Opel Lotus, winning the Benelux Championship in 1992. Later that year, he also won the EFDA Nations Cup, representing the Netherlands. Progressing to German Formula Three in 1993, Verstappen won the title in his rookie season with Opel, further winning the Masters of Formula 3.\n",
      "Signing for Benetton as a test driver in 1994, Verstappen made his Formula One debut at the Brazilian Grand Prix, replacing the injured JJ Lehto to partner Michael Schumacher for the opening two rounds of the season. Verstappen formally replaced Lehto after the Canadian Grand Prix,\u001b[0mPage: Max Verstappen\n",
      "Summary: Max Emilian Verstappen (Dutch pronunciation: [ˈmɑks fɛrˈstɑpə(n)]; born 30 September 1997) is a Dutch and Belgian racing driver who competes under the Dutch flag in Formula One for Red Bull Racing. Verstappen has won four Formula One World Drivers' Championship titles, which he won consecutively from 2021 to 2024 with Red Bull, and has won 65 Grands Prix across 11 seasons.\n",
      "Born in Hasselt and raised in Maaseik, Verstappen is the son of Dutch former Formula One driver Jos Verstappen and Belgian former kart racer Sophie Kumpen. After a successful karting career—culminating in his record-breaking 2013 season—Verstappen graduated to junior formulae. Progressing directly to FIA European Formula 3, Verstappen broke several records on his way to third in the championship in his rookie season with Van Amersfoort. Aged 17, Verstappen signed for Toro Rosso in 2015 as part of the Red Bull Junior Team, becoming the youngest driver in Formula One history at the Australian Grand Prix. Following several points finishes in his debut season, Verstappen retained his seat for 2016 before being promoted to parent team Red Bull after four rounds. On debut for Red Bull, aged 18, Verstappen won the Spanish Grand Prix, becoming the youngest-ever driver to win a Formula One Grand Prix. Verstappen achieved multiple race wins in his 2017 and 2018 campaigns, before finishing third in both the 2019 and 2020 World Drivers' Championships under Honda power.\n",
      "Verstappen won his maiden title in 2021 after overtaking Lewis Hamilton on the final lap of the last race of the season, becoming the first World Drivers' Champion from the Netherlands. Verstappen won the next two championships in 2022 and 2023, overturning the largest points deficit in Formula One history in the former and breaking numerous records across both seasons. He secured his fourth consecutive title in 2024 after winning nine Grands Prix, including a widely acclaimed wet-weather performance in São Paulo, to become the first driver to win the championship driving for a third-placed constructor in 41 years.\n",
      "As of the 2025 Hungarian Grand Prix, Verstappen has achieved 65 race wins, 44 pole positions, 34 fastest laps, and 117 podiums in Formula One. In addition to being the youngest Grand Prix winner, he holds several Formula One records, including the most wins in a season (19), the most podium finishes in a season (21), the most consecutive wins (10), and the most consecutive pole positions (8, shared with Ayrton Senna). Verstappen is contracted to remain at Red Bull until at least the end of the 2028 season. He has also competed professionally in sim racing since 2015, winning several marquee iRacing events. Verstappen was listed in the 2024 issue of Time as one of the 100 most influential people globally, and was appointed an Officer of the Order of Orange-Nassau in 2022.\n",
      "\n",
      "Page: Jos Verstappen\n",
      "Summary: Johannes Franciscus \"Jos\" Verstappen (Dutch pronunciation: [ˈjɔs fɛrˈstɑpə(n)]; born 4 March 1972) is a Dutch racing and rally driver, who competes in the European Rally Championship as a privateer. Verstappen competed in Formula One between 1994 and 2003.\n",
      "Born and raised in Dutch Limburg, Verstappen began competitive kart racing aged eight. After a successful karting career—culminating in two direct-drive Karting European Championships in 1989—Verstappen graduated to junior formulae. He started his career in Formula Opel Lotus, winning the Benelux Championship in 1992. Later that year, he also won the EFDA Nations Cup, representing the Netherlands. Progressing to German Formula Three in 1993, Verstappen won the title in his rookie season with Opel, further winning the Masters of Formula 3.\n",
      "Signing for Benetton as a test driver in 1994, Verstappen made his Formula One debut at the Brazilian Grand Prix, replacing the injured JJ Lehto to partner Michael Schumacher for the opening two rounds of the season. Verstappen formally replaced Lehto after the Canadian Grand Prix,\n"
     ]
    }
   ],
   "source": [
    "print(tool.run(\"Who is Max Verstappen?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98445798",
   "metadata": {},
   "source": [
    "## Create your custom tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11c98b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Get the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "\n",
    "# We have to define a docstring for the tool to work properly, without it, it won't show up in the list of tools the agent can use and give error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d53f964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_length.invoke(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b57314fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5e6e6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_numbers.invoke({\"a\": 5, \"b\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820f4583",
   "metadata": {},
   "source": [
    "## Concept of Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b448e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae311999",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools([\"wikipedia\"], llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be3e276b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p_/v95prvpj4cz7y53gvmgm1d_r0000gn/T/ipykernel_90961/1431667125.py:1: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2f7e27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p_/v95prvpj4cz7y53gvmgm1d_r0000gn/T/ipykernel_90961/1252314138.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  agent.run(\"Hello how are you?\")\n",
      "/Users/atharvabot7/Downloads/MLOPS-Tutorials/LangGraph/langgraph/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI am an AI, I do not have feelings. I am ready to help you with your questions.\n",
      "Final Answer: I am an AI, I do not have feelings. I am ready to help you with your questions.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am an AI, I do not have feelings. I am ready to help you with your questions.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Hello how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ac0102b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atharvabot7/Downloads/MLOPS-Tutorials/LangGraph/langgraph/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: I need to find out who the current Formula 1 world champion is. This is a factual question that can be answered by looking up the results of the most recent Formula 1 season. The Wikipedia tool is well-suited for this. I will search for \"Formula One World Champion\" or \"List of Formula One World Champions\".\n",
      "Action: wikipedia\n",
      "Action Input: List of Formula One World Champions\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Formula One World Champion\n",
      "Summary: A Formula One World Champion is a racing driver or automobile constructor which has been designated such a title by the governing body of Formula One - the Fédération Internationale de l'Automobile (FIA). Every Formula One World Champion since the inaugural World Drivers' Championship in 1950 and the inaugural World Constructors' Championship in 1958 has been awarded the title by accumulating the required points during the course of the F1 season of that particular year, by participating in relevant Grands Prix.\n",
      "The winner of the FIA Formula One World Drivers' Championship (WDC) is the most successful Formula One driver over a particular season, as determined by a points system based on individual Grand Prix results. Lewis Hamilton and Michael Schumacher hold the joint record for most Drivers' Championships with seven apiece.\n",
      "The winner of the Formula One World Constructors' Championship (WCC) is the most successful Formula One constructor over a particular season, as determined by a points system. The Constructors' Championship was first awarded, as the International Cup for F1 Manufacturers, in 1958 to Vanwall. Ferrari hold the record for most Constructors' Championships with 16.\n",
      "\n",
      "Page: List of Formula One World Drivers' Champions\n",
      "Summary: Formula One, abbreviated to F1, is the highest class of open-wheeled auto racing defined by the Fédération Internationale de l'Automobile (FIA), motorsport's world governing body. The \"formula\" in the name refers to a set of rules to which all participants and cars must conform. The Formula One World Championship season consists of a series of races, known as Grands Prix, held usually on purpose-built circuits, and in a few cases on closed city streets. The World Drivers' Championship is presented by the FIA to the most successful Formula One driver over the course of the season through a points system based on individual Grand Prix results. The World Championship is won by a driver when it is no longer mathematically possible for another competitor to overtake their points total regardless of the outcome of the remaining races, although it is not officially awarded until the FIA Prize Giving Ceremony (held in various cities in different years) following the conclusion of the season.\n",
      "Michael Schumacher and Lewis Hamilton hold the record for the most World Drivers' championships, both having won the title on seven occasions. Juan Manuel Fangio is third with five titles. Schumacher also holds the record for the most consecutive World Drivers' titles with five between the 2000 and the 2004 seasons. Nigel Mansell holds the record of competing in the highest number of seasons before winning the World Drivers' Championship, entering Formula One in 1980 and achieving the title in 1992, a span of 13 seasons. Nico Rosberg has the highest number of Grand Prix starts before winning his first title, a period of 206 Grands Prix between the 2006 Bahrain and the 2016 Abu Dhabi Grand Prix. Sebastian Vettel is the youngest winner of the World Drivers' Championship; he was 23 years and 134 days old when he won the 2010 championship. Juan Manuel Fangio is the oldest winner of the World Drivers' Championship; he was 46 years and 41 days old when he won the 1957 title.\n",
      "As of the 2024 season, out of the 777 drivers who have started a Formula One Grand Prix, the 75 titles awarded have been won by a total of 34 different drivers. The first Formula One World Drivers' Champion was Giuseppe Farina in the 1950 championship and the current title holder is Max Verstappen in the 2024 season. The title has been won by drivers from the United Kingdom 20 times between 10 drivers, more than any other nation, followed by Brazil, Finland and Germany with three drivers each. The title has been won by drivers from Scuderia Ferrari 15 times between 9 drivers, more than any other team, followed by McLaren with 12 titles between 7 drivers. The Drivers' Championship has been won in the fina\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atharvabot7/Downloads/MLOPS-Tutorials/LangGraph/langgraph/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The current Formula 1 World Champion is Max Verstappen, who won the title in the 2023 season.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current Formula 1 World Champion is Max Verstappen, who won the title in the 2023 season.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Who is the current world champion in Formula 1 racing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57bb2515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atharvabot7/Downloads/MLOPS-Tutorials/LangGraph/langgraph/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI need to find out what GPT-2 is. I will use Wikipedia to get a definition and some context.\n",
      "Action: wikipedia\n",
      "Action Input: \"GPT-2\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: GPT-2\n",
      "Summary: Generative Pre-trained Transformer 2 (GPT-2) is a large language model by OpenAI and the second in their foundational series of GPT models. GPT-2 was pre-trained on a dataset of 8 million web pages. It was partially released in February 2019, followed by full release of the 1.5-billion-parameter model on November 5, 2019.\n",
      "GPT-2 was created as a \"direct scale-up\" of GPT-1 with a ten-fold increase in both its parameter count and the size of its training dataset. It is a general-purpose learner and its ability to perform the various tasks was a consequence of its general ability to accurately predict the next item in a sequence, which enabled it to translate texts, answer questions about a topic from a text, summarize passages from a larger text, and generate text output on a level sometimes indistinguishable from that of humans; however, it could become repetitive or nonsensical when generating long passages. It was superseded by the GPT-3 and GPT-4 models, which are no longer open source.\n",
      "GPT-2 has, like its predecessor GPT-1 and its successors GPT-3 and GPT-4, a generative pre-trained transformer architecture, implementing a deep neural network, specifically a transformer model, which uses attention instead of older recurrence- and convolution-based architectures. Attention mechanisms allow the model to selectively focus on segments of input text it predicts to be the most relevant. This model allows for greatly increased parallelization, and outperforms previous benchmarks for RNN/CNN/LSTM-based models.\n",
      "\n",
      "Page: GPT-3\n",
      "Summary: Generative Pre-trained Transformer 3 (GPT-3) is a large language model released by OpenAI in 2020.\n",
      "Like its predecessor, GPT-2, it is a decoder-only transformer model of deep neural network, which supersedes recurrence and convolution-based architectures with a technique known as \"attention\". This attention mechanism allows the model to focus selectively on segments of input text it predicts to be most relevant. GPT-3 has 175 billion parameters, each with 16-bit precision, requiring 350GB of storage since each parameter occupies 2 bytes. It has a context window size of 2048 tokens, and has demonstrated strong \"zero-shot\" and \"few-shot\" learning abilities on many tasks.\n",
      "On September 22, 2020, Microsoft announced that it had licensed GPT-3 exclusively. Others can still receive output from its public API, but only Microsoft has access to the underlying model.\n",
      "\n",
      "Page: GPT-4\n",
      "Summary: Generative Pre-trained Transformer 4 (GPT-4) is a large language model developed by OpenAI and the fourth in its series of GPT foundation models. It was launched on March 14, 2023, and was publicly accessible through the chatbot products ChatGPT and Microsoft Copilot until 2025; it is currently available via OpenAI's API.  \n",
      "GPT-4 is more capable than its predecessor GPT-3.5. GPT-4 Vision (GPT-4V) is a version of GPT-4 that can process images in addition to text. OpenAI has not revealed technical details and statistics about GPT-4, such as the precise size of the model.\n",
      "GPT-4, as a generative pre-trained transformer (GPT), was first trained to predict the next token for a large amount of text (both public data and \"data licensed from third-party providers\"). Then, it was fine-tuned for human alignment and policy compliance, notably with reinforcement learning from human feedback (RLHF).\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atharvabot7/Downloads/MLOPS-Tutorials/LangGraph/langgraph/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:357: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `Generative Pre-trained Transformer 2 (GPT-2) is a large language model developed by OpenAI, released in 2019. It is the second model in their GPT series and is known for its ability to generate human-like text.\n\nKey points about GPT-2:\n*   **Architecture:** It uses a transformer-based neural network architecture, which was a significant improvement over previous designs.\n*   **Training:** It was pre-trained on a large dataset of 8 million web pages.\n*   **Capabilities:** By predicting the next word in a sequence, GPT-2 can perform a variety of natural language tasks, including text generation, summarization, translation, and answering questions, without being explicitly trained for each task.\n*   **Release:** OpenAI initially released a smaller version of the model due to concerns about potential misuse for generating fake news or spam. The full 1.5-billion-parameter model was eventually released in November 2019.\n*   **Successors:** GPT-2 was a direct predecessor to the more powerful and larger models, GPT-3 and GPT-4.`\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m~/Downloads/MLOPS-Tutorials/LangGraph/langgraph/lib/python3.10/site-packages/langchain/agents/agent.py:1352\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1351\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m-> 1352\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_action_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Downloads/MLOPS-Tutorials/LangGraph/langgraph/lib/python3.10/site-packages/langchain/agents/agent.py:801\u001b[0m, in \u001b[0;36mAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    800\u001b[0m full_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfull_inputs)\n\u001b[0;32m--> 801\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/MLOPS-Tutorials/LangGraph/langgraph/lib/python3.10/site-packages/langchain/agents/mrkl/output_parser.py:80\u001b[0m, in \u001b[0;36mMRKLOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     79\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse LLM output: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[1;32m     81\u001b[0m         msg,\n\u001b[1;32m     82\u001b[0m         observation\u001b[38;5;241m=\u001b[39mMISSING_ACTION_AFTER_THOUGHT_ERROR_MESSAGE,\n\u001b[1;32m     83\u001b[0m         llm_output\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m     84\u001b[0m         send_to_llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*Action\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*Input\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*:[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]*(.*)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     88\u001b[0m     text,\n\u001b[1;32m     89\u001b[0m     re\u001b[38;5;241m.\u001b[39mDOTALL,\n\u001b[1;32m     90\u001b[0m ):\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Could not parse LLM output: `Generative Pre-trained Transformer 2 (GPT-2) is a large language model developed by OpenAI, released in 2019. It is the second model in their GPT series and is known for its ability to generate human-like text.\n\nKey points about GPT-2:\n*   **Architecture:** It uses a transformer-based neural network architecture, which was a significant improvement over previous designs.\n*   **Training:** It was pre-trained on a large dataset of 8 million web pages.\n*   **Capabilities:** By predicting the next word in a sequence, GPT-2 can perform a variety of natural language tasks, including text generation, summarization, translation, and answering questions, without being explicitly trained for each task.\n*   **Release:** OpenAI initially released a smaller version of the model due to concerns about potential misuse for generating fake news or spam. The full 1.5-billion-parameter model was eventually released in November 2019.\n*   **Successors:** GPT-2 was a direct predecessor to the more powerful and larger models, GPT-3 and GPT-4.`\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is GPT-2?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/MLOPS-Tutorials/LangGraph/langgraph/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:190\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    189\u001b[0m     emit_warning()\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/MLOPS-Tutorials/LangGraph/langgraph/lib/python3.10/site-packages/langchain/chains/base.py:627\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    626\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m--> 627\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    628\u001b[0m         _output_key\n\u001b[1;32m    629\u001b[0m     ]\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    633\u001b[0m         _output_key\n\u001b[1;32m    634\u001b[0m     ]\n",
      "File \u001b[0;32m~/Downloads/MLOPS-Tutorials/LangGraph/langgraph/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:190\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    189\u001b[0m     emit_warning()\n\u001b[0;32m--> 190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/MLOPS-Tutorials/LangGraph/langgraph/lib/python3.10/site-packages/langchain/chains/base.py:410\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    403\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    408\u001b[0m }\n\u001b[0;32m--> 410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/MLOPS-Tutorials/LangGraph/langgraph/lib/python3.10/site-packages/langchain/chains/base.py:165\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    164\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 165\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    170\u001b[0m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    171\u001b[0m         inputs,\n\u001b[1;32m    172\u001b[0m         outputs,\n\u001b[1;32m    173\u001b[0m         return_only_outputs,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Downloads/MLOPS-Tutorials/LangGraph/langgraph/lib/python3.10/site-packages/langchain/agents/agent.py:1625\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1624\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1625\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1632\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1633\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m   1634\u001b[0m             next_step_output,\n\u001b[1;32m   1635\u001b[0m             intermediate_steps,\n\u001b[1;32m   1636\u001b[0m             run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1637\u001b[0m         )\n",
      "File \u001b[0;32m~/Downloads/MLOPS-Tutorials/LangGraph/langgraph/lib/python3.10/site-packages/langchain/agents/agent.py:1325\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_take_next_step\u001b[39m(\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1318\u001b[0m     name_to_tool_map: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, BaseTool],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[AgentFinish, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[AgentAction, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consume_next_step(\n\u001b[0;32m-> 1325\u001b[0m         \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m                \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1334\u001b[0m     )\n",
      "File \u001b[0;32m~/Downloads/MLOPS-Tutorials/LangGraph/langgraph/lib/python3.10/site-packages/langchain/agents/agent.py:1369\u001b[0m, in \u001b[0;36mAgentExecutor._iter_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_error:\n\u001b[1;32m   1363\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn output parsing error occurred. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1365\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to pass this error back to the agent and have it try \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1366\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magain, pass `handle_parsing_errors=True` to the AgentExecutor. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1367\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is the error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1368\u001b[0m     )\n\u001b[0;32m-> 1369\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1370\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse LLM output: `Generative Pre-trained Transformer 2 (GPT-2) is a large language model developed by OpenAI, released in 2019. It is the second model in their GPT series and is known for its ability to generate human-like text.\n\nKey points about GPT-2:\n*   **Architecture:** It uses a transformer-based neural network architecture, which was a significant improvement over previous designs.\n*   **Training:** It was pre-trained on a large dataset of 8 million web pages.\n*   **Capabilities:** By predicting the next word in a sequence, GPT-2 can perform a variety of natural language tasks, including text generation, summarization, translation, and answering questions, without being explicitly trained for each task.\n*   **Release:** OpenAI initially released a smaller version of the model due to concerns about potential misuse for generating fake news or spam. The full 1.5-billion-parameter model was eventually released in November 2019.\n*   **Successors:** GPT-2 was a direct predecessor to the more powerful and larger models, GPT-3 and GPT-4.`\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
     ]
    }
   ],
   "source": [
    "agent.run(\"What is GPT-2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957078ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# Tavily basically provides a search engine tool that can be used to fetch real-time information from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5177f5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9232aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.run(\"Who is the current world champion in Formula 1 racing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6e09f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.invoke(\"Who is the current world champion in Formula 1 racing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587a7a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edb30ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc00aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5932eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da525d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_tool_calling_agent\n",
    "\n",
    "agent1 = create_tool_calling_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61724b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d724aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    max_iterations=6,\n",
    "    early_stopping_method=\"generate\",   # <- if it’s stuck, ask LLM to write a final answer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70e2621",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"Who is John Wick?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b7edda",
   "metadata": {},
   "source": [
    "## Self Ask with Search Agent Langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf945ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb5d59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/John_Wick\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b0006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031885ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122a940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector  = FAISS.from_documents(documents, embeddings)\n",
    "retriever = vector.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0cd26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"Who is John Wick?\")[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764c9ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315d16de",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool = create_retriever_tool(\n",
    "    retriever=retriever,\n",
    "    name=\"JohnWickRetriever\",\n",
    "    description=\"Useful for when you need to find information about John Wick from Wikipedia.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a287b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools1 = [search, retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d28aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain.agents import AgentExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfbd705",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent2 = create_tool_calling_agent(llm, tools1, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1004e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent=agent2, tools=tools1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ec874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"What is Johns profession?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e25cc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.invoke({\"input\": \"What is the weather in New York?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18df25f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "search2 = TavilySearchResults(k=3)\n",
    "tool2 = [search2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f906666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent3 = create_tool_calling_agent(llm, tool2, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e037871",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor1 = AgentExecutor(agent=agent3, tools=tool2, verbose=True, max_iterations=4, early_stopping_method=\"generate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5fa3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor1.invoke({\"input\": \"What is the weather in New York as of 29th August 2025?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f34756f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "525091b4",
   "metadata": {},
   "source": [
    "## ReAct Agent (Reasoning and Action Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "381d52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f601dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fe6966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991fecdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0604f1ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa6620b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9230cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3d65f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
